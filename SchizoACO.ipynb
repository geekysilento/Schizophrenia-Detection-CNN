{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/geekysilento/Schizophrenia-Detection-CNN/blob/main/Schizophrenia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and setting up the Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a was unexpected at this time.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "! for a in /sys/bus/pci/devices/*; do echo 0 | sudo tee -a $a/numa_node; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading CSV Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>group</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>44</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>39</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>53</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>52</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>41</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>28</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>32</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>37</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>33</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject   group  gender   age   education\n",
       "0         1       0       M    44        16.0\n",
       "1         2       0       M    39        17.0\n",
       "2         3       0       M    53        18.0\n",
       "3         4       0       M    52        15.0\n",
       "4         5       0       M    41        16.0\n",
       "..      ...     ...     ...   ...         ...\n",
       "76       77       1       M    28        13.0\n",
       "77       78       1       F    32        16.0\n",
       "78       79       1       M    37        16.0\n",
       "79       80       1       M    33        13.0\n",
       "80       81       1       M    56        13.0\n",
       "\n",
       "[81 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demographic = pd.read_csv(\"button-tone-sz/demographic.csv\")\n",
    "demographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>time_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1500.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-1499.0234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-1498.0469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-1497.0703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-1496.0938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3067</th>\n",
       "      <td>3068</td>\n",
       "      <td>1495.1172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3068</th>\n",
       "      <td>3069</td>\n",
       "      <td>1496.0938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069</th>\n",
       "      <td>3070</td>\n",
       "      <td>1497.0703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3070</th>\n",
       "      <td>3071</td>\n",
       "      <td>1498.0469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3071</th>\n",
       "      <td>3072</td>\n",
       "      <td>1499.0234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3072 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample    time_ms\n",
       "0          1 -1500.0000\n",
       "1          2 -1499.0234\n",
       "2          3 -1498.0469\n",
       "3          4 -1497.0703\n",
       "4          5 -1496.0938\n",
       "...      ...        ...\n",
       "3067    3068  1495.1172\n",
       "3068    3069  1496.0938\n",
       "3069    3070  1497.0703\n",
       "3070    3071  1498.0469\n",
       "3071    3072  1499.0234\n",
       "\n",
       "[3072 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = pd.read_csv(\"button-tone-sz/time.csv\")\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1, 34: 1, 35: 1, 36: 1, 37: 1, 38: 1, 39: 1, 40: 1, 41: 1, 42: 1, 43: 1, 44: 1, 45: 1, 46: 1, 47: 1, 48: 1, 49: 1, 50: 1, 51: 1, 52: 1, 53: 1, 54: 1, 55: 1, 56: 1, 57: 1, 58: 1, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 1, 68: 1, 69: 1, 70: 1, 71: 1, 72: 1, 73: 1, 74: 1, 75: 1, 76: 1, 77: 1, 78: 1, 79: 1, 80: 1, 81: 1}\n",
      "Electrodes List \n",
      " ['Fp1', 'AF7', 'AF3', 'F1', 'F3', 'F5', 'F7', 'FT7', 'FC5', 'FC3', 'FC1', 'C1', 'C3', 'C5', 'T7', 'TP7', 'CP5', 'CP3', 'CP1', 'P1', 'P3', 'P5', 'P7', 'P9', 'PO7', 'PO3', 'O1', 'Iz', 'Oz', 'POz', 'Pz', 'CPz', 'Fpz', 'Fp2', 'AF8', 'AF4', 'AFz', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FT8', 'FC6', 'FC4', 'FC2', 'FCz', 'Cz', 'C2', 'C4', 'C6', 'T8', 'TP8', 'CP6', 'CP4', 'CP2', 'P2', 'P4', 'P6', 'P8', 'P10', 'PO8', 'PO4', 'O2', 'VEOa', 'VEOb', 'HEOL', 'HEOR', 'Nose', 'TP10']\n"
     ]
    }
   ],
   "source": [
    "diagnosis_dict = dict(zip(demographic.subject, demographic[\" group\"]))\n",
    "electrodes_list = list(pd.read_csv(\"button-tone-sz/columnLabels.csv\").columns[4:])\n",
    "\n",
    "print(diagnosis_dict)\n",
    "\n",
    "\n",
    "print(\"Electrodes List \\n\",electrodes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7f7965537640e7b0e2e387485d7612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[17], line 29\u001b[0m\n",
      "\u001b[0;32m     27\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(csv_path):\n",
      "\u001b[0;32m     28\u001b[0m     csv_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpart2_path\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mperson_number\u001b[39m}\u001b[39;00m\u001b[39m.csv/\u001b[39m\u001b[39m{\u001b[39;00mperson_number\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m---> 29\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(csv_path,\n",
      "\u001b[0;32m     30\u001b[0m             header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n",
      "\u001b[0;32m     31\u001b[0m             names\u001b[39m=\u001b[39;49mcolumn_list)\n",
      "\u001b[0;32m     32\u001b[0m trials_list \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(df\u001b[39m.\u001b[39mtrial)\n",
      "\u001b[0;32m     35\u001b[0m \u001b[39mfor\u001b[39;00m t1, trial_number \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trials_list):\n",
      "\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n",
      "\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n",
      "\u001b[0;32m    936\u001b[0m     dialect,\n",
      "\u001b[0;32m    937\u001b[0m     delimiter,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n",
      "\u001b[0;32m    945\u001b[0m )\n",
      "\u001b[0;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n",
      "\u001b[1;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n",
      "\u001b[0;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "\u001b[0;32m    616\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n",
      "\u001b[1;32m--> 617\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n",
      "\u001b[0;32m   1741\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n",
      "\u001b[0;32m   1742\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;32m   1743\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n",
      "\u001b[0;32m   1744\u001b[0m     (\n",
      "\u001b[0;32m   1745\u001b[0m         index,\n",
      "\u001b[0;32m   1746\u001b[0m         columns,\n",
      "\u001b[0;32m   1747\u001b[0m         col_dict,\n",
      "\u001b[1;32m-> 1748\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n",
      "\u001b[0;32m   1749\u001b[0m         nrows\n",
      "\u001b[0;32m   1750\u001b[0m     )\n",
      "\u001b[0;32m   1751\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "\u001b[0;32m   1752\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n",
      "\u001b[0;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n",
      "\u001b[1;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n",
      "\u001b[0;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n",
      "\u001b[0;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\n",
      "File \u001b[1;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "\n",
      "File \u001b[1;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "def averaged_by_N_rows(a, n):\n",
    "    \"\"\"\n",
    "    A function that averages n rows in a matrix\n",
    "    \"\"\"\n",
    "    shape = a.shape\n",
    "    assert len(shape) == 2\n",
    "    assert shape[0] % n == 0\n",
    "    b = a.reshape(shape[0] // n, n,\n",
    "                  shape[1])\n",
    "    mean_vec = b.mean(axis=1)\n",
    "    return mean_vec\n",
    "\n",
    "N_AVERAGED = 16\n",
    "x_counter = 0\n",
    "X = np.zeros((81 * 100,  9216 * len(electrodes_list) // N_AVERAGED), dtype=\"float32\")\n",
    "Y = np.zeros(len(X))\n",
    "\n",
    "part1_path = \"button-tone-sz\"\n",
    "part2_path = \"button-tone-sz\"\n",
    "\n",
    "\n",
    "column_list = pd.read_csv(\"button-tone-sz/columnLabels.csv\").columns\n",
    "for person_number in tqdm(range(1, 81 + 1)):\n",
    "\n",
    "\n",
    "    csv_path = f\"{part1_path}/{person_number}.csv/{person_number}.csv\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        csv_path = f\"{part2_path}/{person_number}.csv/{person_number}.csv\"\n",
    "    df = pd.read_csv(csv_path,\n",
    "                header=None,\n",
    "                names=column_list)\n",
    "    trials_list = set(df.trial)\n",
    "\n",
    "\n",
    "    for t1, trial_number in enumerate(trials_list):\n",
    "        number_of_trials = len(df[df.trial == trial_number])\n",
    "        if number_of_trials == 9216.0:\n",
    "            current_sample_matrix = df[df.trial == trial_number][electrodes_list].values\n",
    "            averaged_by_N = averaged_by_N_rows(current_sample_matrix, n=N_AVERAGED)\n",
    "            averaged_by_N_big_vec = averaged_by_N.reshape(-1)\n",
    "            X[x_counter] = averaged_by_N_big_vec.astype(np.float32)\n",
    "            Y[x_counter] = diagnosis_dict[person_number]\n",
    "            x_counter += 1\n",
    "print(\"Total trials with the Appropriate number of measurements - \", x_counter)\n",
    "X = X[: x_counter]\n",
    "Y = Y[: x_counter]\n",
    "\n",
    "print(\"Total trials with the Appropriate number of measurements- \", x_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=42)\n",
    "X_norm = (normalize(X.reshape(-1, 70), axis=0, norm='max')).reshape(X.shape)\n",
    "X_train_norm, X_test_norm, Y_train_norm, Y_test_norm = train_test_split(X_norm, Y, test_size=0.2, shuffle=True, random_state=42)\n",
    "_norm = X\n",
    "\n",
    "X_train = X_train_norm.reshape(X_train_norm.shape[0], -1, len(electrodes_list))\n",
    "X_test = X_test_norm.reshape(X_test_norm.shape[0], -1, len(electrodes_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[22], line 67\u001b[0m\n",
      "\u001b[0;32m     62\u001b[0m \u001b[39mfor\u001b[39;00m iteration \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iterations):\n",
      "\u001b[0;32m     63\u001b[0m     params \u001b[39m=\u001b[39m {\n",
      "\u001b[0;32m     64\u001b[0m         param_name: np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(param_values) \u001b[39mfor\u001b[39;00m param_name, param_values \u001b[39min\u001b[39;00m search_space\u001b[39m.\u001b[39mitems()\n",
      "\u001b[0;32m     65\u001b[0m     }\n",
      "\u001b[1;32m---> 67\u001b[0m     val_loss \u001b[39m=\u001b[39m evaluate_hyperparameters(params)\n",
      "\u001b[0;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m val_loss \u001b[39m<\u001b[39m best_val_loss:\n",
      "\u001b[0;32m     70\u001b[0m         best_params \u001b[39m=\u001b[39m params\n",
      "\n",
      "Cell \u001b[1;32mIn[22], line 37\u001b[0m, in \u001b[0;36mevaluate_hyperparameters\u001b[1;34m(params)\u001b[0m\n",
      "\u001b[0;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_hyperparameters\u001b[39m(params):\n",
      "\u001b[1;32m---> 37\u001b[0m     model \u001b[39m=\u001b[39m create_model(params)\n",
      "\u001b[0;32m     38\u001b[0m     early_stopping \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;32m     40\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(X_train, Y_train, validation_data\u001b[39m=\u001b[39m(X_test, Y_test), epochs\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, batch_size\u001b[39m=\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m], callbacks\u001b[39m=\u001b[39m[early_stopping])\n",
      "\n",
      "Cell \u001b[1;32mIn[22], line 13\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(params)\u001b[0m\n",
      "\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_model\u001b[39m(params):\n",
      "\u001b[0;32m     11\u001b[0m     model \u001b[39m=\u001b[39m Sequential()\n",
      "\u001b[1;32m---> 13\u001b[0m     model\u001b[39m.\u001b[39madd(LSTM(params[\u001b[39m'\u001b[39m\u001b[39mlstm_units\u001b[39m\u001b[39m'\u001b[39m], input_shape\u001b[39m=\u001b[39m(X_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], X_train\u001b[39m.\u001b[39;49mshape[\u001b[39m2\u001b[39;49m]), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtanh\u001b[39m\u001b[39m'\u001b[39m, return_sequences\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
      "\u001b[0;32m     14\u001b[0m     model\u001b[39m.\u001b[39madd(Dropout(params[\u001b[39m'\u001b[39m\u001b[39mdropout_rate\u001b[39m\u001b[39m'\u001b[39m]))\n",
      "\u001b[0;32m     16\u001b[0m     model\u001b[39m.\u001b[39madd(LSTM(params[\u001b[39m'\u001b[39m\u001b[39mlstm_units\u001b[39m\u001b[39m'\u001b[39m], activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtanh\u001b[39m\u001b[39m'\u001b[39m, return_sequences\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
      "\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model\n",
    "def create_model(params):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(params['lstm_units'], input_shape=(X_train.shape[1], X_train.shape[2]), activation='tanh', return_sequences=True))\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    model.add(LSTM(params['lstm_units'], activation='tanh', return_sequences=True))\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    model.add(GRU(params['lstm_units'], activation='tanh'))\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    model.add(Dense(params['dense_units'], activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(params['dense_units'], activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    custom_optimizer = Adam(learning_rate=params['learning_rate'])\n",
    "    model.compile(loss='binary_crossentropy', optimizer=custom_optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define your objective function for hyperparameter optimization\n",
    "def evaluate_hyperparameters(params):\n",
    "    model = create_model(params)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=2, batch_size=params['batch_size'], callbacks=[early_stopping])\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "search_space = {\n",
    "    'lstm_units': [128, 192, 256],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'dense_units': [128, 256, 512],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size': [32, 64, 128]\n",
    "}\n",
    "\n",
    "# Split your data into training and validation sets\n",
    "# X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform random search for hyperparameter optimization\n",
    "num_iterations = 20\n",
    "best_params = None\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    params = {\n",
    "        param_name: np.random.choice(param_values) for param_name, param_values in search_space.items()\n",
    "    }\n",
    "    \n",
    "    val_loss = evaluate_hyperparameters(params)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_params = params\n",
    "        best_val_loss = val_loss\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "final_model = create_model(best_params)\n",
    "final_model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, batch_size=best_params['batch_size'], callbacks=[early_stopping])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
